{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a38f84e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1dd07bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç UNDERSTANDING CACHE AND STORAGE\n",
      "==================================================\n",
      "=== DEFAULT CACHE LOCATIONS ===\n",
      "Your HOME directory: /home/an3854\n",
      "Home directory size: 37.30 GB\n",
      "\n",
      "HuggingFace Cache Variables:\n",
      "  HF_HOME: /scratch/an3854/huggingface_cache\n",
      "  HF_CACHE_HOME: Not set (will use HF_HOME)\n",
      "  TRANSFORMERS_CACHE: Not set (will use HF_HOME)\n",
      "\n",
      "=== EXISTING CACHE CONTENTS ===\n",
      "  HuggingFace Hub: Does not exist\n",
      "  HuggingFace Transformers: Does not exist\n",
      "  PyTorch Hub: Does not exist\n",
      "  General Cache: Does not exist\n",
      "\n",
      "=== DISK SPACE ===\n",
      "HOME directory:\n",
      "  Total: 15117.5 GB\n",
      "  Used: 11604.7 GB\n",
      "  Free: 2857.4 GB\n",
      "  Usage: 76.8%\n",
      "\n",
      "SCRATCH directory (/scratch/an3854):\n",
      "  Free: 5594529.5 GB\n",
      "  Total: 7847311.7 GB\n",
      "\n",
      "=== WHAT HAPPENS WHEN LOADING MODELS ===\n",
      "\n",
      "1. HuggingFace AutoModel.from_pretrained():\n",
      "   - Downloads model files to ~/.cache/huggingface/hub/\n",
      "   - Each model gets a folder like 'models--Qwen--Qwen-7B-Chat'\n",
      "   - Files include: pytorch_model.bin, config.json, tokenizer files\n",
      "   - These can be 7-14GB+ PER MODEL\n",
      "\n",
      "2. vLLM LLM() loading:\n",
      "   - Loads the model files from HuggingFace cache\n",
      "   - Loads model weights into GPU memory\n",
      "   - May create additional cache files\n",
      "\n",
      "3. When you create model copies (for ablation):\n",
      "   - copy.deepcopy() duplicates the ENTIRE model in RAM\n",
      "   - This can be 7-14GB+ in system RAM\n",
      "\n",
      "4. When you save ablated models:\n",
      "   - model.save_pretrained() saves to disk\n",
      "   - Your code saves to './ablated_model_*' directories\n",
      "   - Each ablated model is ANOTHER 7-14GB on disk\n",
      "\n",
      "==================================================\n",
      "ü§î LIKELY ISSUES:\n",
      "1. HuggingFace cache filling up your HOME directory\n",
      "2. Ablated model checkpoints saving to current directory\n",
      "3. Model copies using too much RAM\n",
      "4. Multiple model versions accumulating\n",
      "\n",
      "Run this script, then run your experiment, then run it again\n",
      "to see what changed!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Simple script to understand where HuggingFace and other caches are stored\n",
    "Run this BEFORE loading any models to see the defaults\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def check_environment_defaults():\n",
    "    \"\"\"Check what the default cache locations are\"\"\"\n",
    "    print(\"=== DEFAULT CACHE LOCATIONS ===\")\n",
    "    \n",
    "    # HuggingFace defaults\n",
    "    home = Path.home()\n",
    "    \n",
    "    print(f\"Your HOME directory: {home}\")\n",
    "    print(f\"Home directory size: {get_folder_size(home):.2f} GB\")\n",
    "    print()\n",
    "    \n",
    "    # Check HuggingFace environment variables\n",
    "    print(\"HuggingFace Cache Variables:\")\n",
    "    hf_home = os.environ.get('HF_HOME')\n",
    "    hf_cache = os.environ.get('HF_CACHE_HOME') \n",
    "    transformers_cache = os.environ.get('TRANSFORMERS_CACHE')\n",
    "    \n",
    "    if hf_home:\n",
    "        print(f\"  HF_HOME: {hf_home}\")\n",
    "    else:\n",
    "        default_hf = home / \".cache\" / \"huggingface\"\n",
    "        print(f\"  HF_HOME: Not set (default: {default_hf})\")\n",
    "        \n",
    "    if hf_cache:\n",
    "        print(f\"  HF_CACHE_HOME: {hf_cache}\")\n",
    "    else:\n",
    "        print(f\"  HF_CACHE_HOME: Not set (will use HF_HOME)\")\n",
    "        \n",
    "    if transformers_cache:\n",
    "        print(f\"  TRANSFORMERS_CACHE: {transformers_cache}\")\n",
    "    else:\n",
    "        print(f\"  TRANSFORMERS_CACHE: Not set (will use HF_HOME)\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "def get_folder_size(folder_path):\n",
    "    \"\"\"Get folder size in GB\"\"\"\n",
    "    try:\n",
    "        total_size = 0\n",
    "        for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "            for filename in filenames:\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                try:\n",
    "                    total_size += os.path.getsize(filepath)\n",
    "                except (OSError, FileNotFoundError):\n",
    "                    continue\n",
    "        return total_size / (1024**3)  # Convert to GB\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def check_existing_caches():\n",
    "    \"\"\"Check what's already in the cache directories\"\"\"\n",
    "    print(\"=== EXISTING CACHE CONTENTS ===\")\n",
    "    \n",
    "    home = Path.home()\n",
    "    cache_locations = [\n",
    "        (\"HuggingFace Hub\", home / \".cache\" / \"huggingface\" / \"hub\"),\n",
    "        (\"HuggingFace Transformers\", home / \".cache\" / \"huggingface\" / \"transformers\"), \n",
    "        (\"PyTorch Hub\", home / \".cache\" / \"torch\" / \"hub\"),\n",
    "        (\"General Cache\", home / \".cache\"),\n",
    "    ]\n",
    "    \n",
    "    for name, path in cache_locations:\n",
    "        if path.exists():\n",
    "            size = get_folder_size(path)\n",
    "            print(f\"  {name}: EXISTS - {size:.2f} GB\")\n",
    "            \n",
    "            # List contents if it's small enough\n",
    "            if size < 1.0:  # Less than 1GB, show contents\n",
    "                try:\n",
    "                    contents = list(path.iterdir())[:5]  # First 5 items\n",
    "                    for item in contents:\n",
    "                        item_size = get_folder_size(item) if item.is_dir() else item.stat().st_size / (1024**3)\n",
    "                        print(f\"    - {item.name}: {item_size:.3f} GB\")\n",
    "                    if len(list(path.iterdir())) > 5:\n",
    "                        print(f\"    ... and {len(list(path.iterdir())) - 5} more items\")\n",
    "                except:\n",
    "                    print(f\"    (Could not list contents)\")\n",
    "        else:\n",
    "            print(f\"  {name}: Does not exist\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "def check_disk_space():\n",
    "    \"\"\"Check disk space in home and scratch\"\"\"\n",
    "    print(\"=== DISK SPACE ===\")\n",
    "    \n",
    "    import shutil\n",
    "    \n",
    "    # Home directory\n",
    "    try:\n",
    "        home_usage = shutil.disk_usage(Path.home())\n",
    "        home_total = home_usage.total / (1024**3)\n",
    "        home_used = home_usage.used / (1024**3) \n",
    "        home_free = home_usage.free / (1024**3)\n",
    "        \n",
    "        print(f\"HOME directory:\")\n",
    "        print(f\"  Total: {home_total:.1f} GB\")\n",
    "        print(f\"  Used: {home_used:.1f} GB\") \n",
    "        print(f\"  Free: {home_free:.1f} GB\")\n",
    "        print(f\"  Usage: {(home_used/home_total)*100:.1f}%\")\n",
    "        \n",
    "        if home_free < 5:\n",
    "            print(\"  ‚ö†Ô∏è  WARNING: Less than 5GB free!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Could not check home disk usage: {e}\")\n",
    "    \n",
    "    # Scratch directory\n",
    "    scratch_dir = f\"/scratch/{os.environ.get('USER', 'unknown')}\"\n",
    "    if os.path.exists(scratch_dir):\n",
    "        try:\n",
    "            scratch_usage = shutil.disk_usage(scratch_dir)\n",
    "            scratch_free = scratch_usage.free / (1024**3)\n",
    "            scratch_total = scratch_usage.total / (1024**3)\n",
    "            print(f\"\\nSCRATCH directory ({scratch_dir}):\")\n",
    "            print(f\"  Free: {scratch_free:.1f} GB\")\n",
    "            print(f\"  Total: {scratch_total:.1f} GB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not check scratch usage: {e}\")\n",
    "    else:\n",
    "        print(f\"\\nSCRATCH directory: {scratch_dir} does not exist\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "def what_happens_when_loading():\n",
    "    \"\"\"Explain what happens when models load\"\"\"\n",
    "    print(\"=== WHAT HAPPENS WHEN LOADING MODELS ===\")\n",
    "    print()\n",
    "    print(\"1. HuggingFace AutoModel.from_pretrained():\")\n",
    "    print(\"   - Downloads model files to ~/.cache/huggingface/hub/\")\n",
    "    print(\"   - Each model gets a folder like 'models--Qwen--Qwen-7B-Chat'\")\n",
    "    print(\"   - Files include: pytorch_model.bin, config.json, tokenizer files\")\n",
    "    print(\"   - These can be 7-14GB+ PER MODEL\")\n",
    "    print()\n",
    "    print(\"2. vLLM LLM() loading:\")\n",
    "    print(\"   - Loads the model files from HuggingFace cache\")\n",
    "    print(\"   - Loads model weights into GPU memory\")\n",
    "    print(\"   - May create additional cache files\")\n",
    "    print()\n",
    "    print(\"3. When you create model copies (for ablation):\")\n",
    "    print(\"   - copy.deepcopy() duplicates the ENTIRE model in RAM\")\n",
    "    print(\"   - This can be 7-14GB+ in system RAM\")\n",
    "    print()\n",
    "    print(\"4. When you save ablated models:\")\n",
    "    print(\"   - model.save_pretrained() saves to disk\")\n",
    "    print(\"   - Your code saves to './ablated_model_*' directories\")\n",
    "    print(\"   - Each ablated model is ANOTHER 7-14GB on disk\")\n",
    "    print()\n",
    "\n",
    "def main():\n",
    "    print(\"üîç UNDERSTANDING CACHE AND STORAGE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    check_environment_defaults()\n",
    "    check_existing_caches() \n",
    "    check_disk_space()\n",
    "    what_happens_when_loading()\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"ü§î LIKELY ISSUES:\")\n",
    "    print(\"1. HuggingFace cache filling up your HOME directory\")\n",
    "    print(\"2. Ablated model checkpoints saving to current directory\")\n",
    "    print(\"3. Model copies using too much RAM\")\n",
    "    print(\"4. Multiple model versions accumulating\")\n",
    "    print()\n",
    "    print(\"Run this script, then run your experiment, then run it again\")\n",
    "    print(\"to see what changed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9586efdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model directly...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:10<00:31, 10.40s/it]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbea705c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecfd3c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abb373e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65f1c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487bb19c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5af788",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f3657",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc5dc9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sciurus-env",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
